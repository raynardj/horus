{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Image Classifcation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going learn [food images](https://www.kaggle.com/kmader/food41) from kaggle\n",
    "\n",
    "### Use the kaggle-cli to download the image\n",
    "\n",
    "```kaggle datasets download -d kmader/food41```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA  = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paperspace'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOME = os.environ[\"HOME\"]\n",
    "HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paperspace/.kaggle/datasets/kmader/food41/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA  = HOME+\"/.kaggle/datasets/kmader/food41/\"\n",
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.3G\\t/home/paperspace/.kaggle/datasets/kmader/food41/food41.zip',\n",
       " '343M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_c101_n1000_r384x384x3.h5',\n",
       " '18M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_c101_n10099_r32x32x1.h5',\n",
       " '30M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_c101_n10099_r32x32x3.h5',\n",
       " '68M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_c101_n10099_r64x64x1.h5',\n",
       " '115M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_c101_n10099_r64x64x3.h5',\n",
       " '25M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_test_c101_n1000_r128x128x1.h5',\n",
       " '44M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_test_c101_n1000_r128x128x3.h5',\n",
       " '1.9M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_test_c101_n1000_r32x32x1.h5',\n",
       " '3.2M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_test_c101_n1000_r32x32x3.h5',\n",
       " '6.7M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_test_c101_n1000_r64x64x1.h5',\n",
       " '12M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_test_c101_n1000_r64x64x3.h5',\n",
       " '4.8G\\t/home/paperspace/.kaggle/datasets/kmader/food41/images.zip',\n",
       " '668K\\t/home/paperspace/.kaggle/datasets/kmader/food41/meta.zip']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!du -sh {DATA}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 11240352\r\n",
      "-rw-rw-r-- 1 paperspace paperspace 5681757245 Aug  2 01:45 \u001b[0m\u001b[01;31mfood41.zip\u001b[0m\r\n",
      "-rw-rw-r-- 1 paperspace paperspace  359407496 Aug  2 01:45 food_c101_n1000_r384x384x3.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace   18236331 Aug  2 01:45 food_c101_n10099_r32x32x1.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace   30452874 Aug  2 01:45 food_c101_n10099_r32x32x3.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace   70546750 Aug  2 01:45 food_c101_n10099_r64x64x1.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace  120306214 Aug  2 01:45 food_c101_n10099_r64x64x3.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace   26042428 Aug  2 01:45 food_test_c101_n1000_r128x128x1.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace   45897229 Aug  2 01:45 food_test_c101_n1000_r128x128x3.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace    1938115 Aug  2 01:45 food_test_c101_n1000_r32x32x1.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace    3294763 Aug  2 01:45 food_test_c101_n1000_r32x32x3.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace    6968764 Aug  2 01:45 food_test_c101_n1000_r64x64x1.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace   11869967 Aug  2 01:45 food_test_c101_n1000_r64x64x3.h5\r\n",
      "drwxrwxr-x 2 paperspace paperspace       4096 Aug  2 11:08 \u001b[01;34mimages\u001b[0m/\r\n",
      "-rw-rw-r-- 1 paperspace paperspace 5132666035 Aug  2 01:45 \u001b[01;31mimages.zip\u001b[0m\r\n",
      "drwxrwxr-x 2 paperspace paperspace       4096 Aug  2 11:04 \u001b[01;34mmeta\u001b[0m/\r\n",
      "-rw-rw-r-- 1 paperspace paperspace     682599 Aug  2 01:45 \u001b[01;31mmeta.zip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# %mkdir -p {DATA}images\n",
    "# %mkdir -p {DATA}meta\n",
    "# %ls -l {DATA}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip and calculate the line of sub-folders: categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['102']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!cd {DATA}/images; unzip ../images.zip > unzip.log;rm -rf unzip.log; ls -l|wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total 4140',\n",
       " '-rw-rw-r-- 1 paperspace paperspace    1184 Jul  9  2014 classes.txt',\n",
       " '-rw-rw-r-- 1 paperspace paperspace    1184 Sep 23  2013 labels.txt',\n",
       " '-rw-rw-r-- 1 paperspace paperspace  566868 Sep 23  2013 test.json',\n",
       " '-rw-rw-r-- 1 paperspace paperspace  489429 Sep 23  2013 test.txt',\n",
       " '-rw-rw-r-- 1 paperspace paperspace 1697751 Sep 21  2013 train.json',\n",
       " '-rw-rw-r-- 1 paperspace paperspace 1468812 Sep 21  2013 train.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!cd {DATA}/meta; unzip -q ../meta.zip; ls -l meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paperspace/.kaggle/datasets/kmader/food41/meta/meta/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "META = DATA + \"meta/meta/\"\n",
    "META"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.json preview ====================\n",
      "{\"churros\": [\"churros/1004234\", \"churros/1013460\",\n",
      "classes.txt preview ====================\n",
      "apple_pie\n",
      "baby_back_ribs\n",
      "baklava\n",
      "beef_carpaccio\n",
      "be\n",
      "train.txt preview ====================\n",
      "apple_pie/1005649\n",
      "apple_pie/1014775\n",
      "apple_pie/1026\n",
      "test.json preview ====================\n",
      "{\"churros\": [\"churros/1061830\", \"churros/1064042\",\n",
      "test.txt preview ====================\n",
      "apple_pie/1011328\n",
      "apple_pie/101251\n",
      "apple_pie/10343\n",
      "labels.txt preview ====================\n",
      "Apple pie\n",
      "Baby back ribs\n",
      "Baklava\n",
      "Beef carpaccio\n",
      "Be\n"
     ]
    }
   ],
   "source": [
    "for m_file in os.listdir(META):\n",
    "    print(m_file, \"preview\",\"=\"*20)\n",
    "    print(open(META+m_file).read()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paperspace/.kaggle/datasets/kmader/food41/images/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG = DATA+\"images/\"\n",
    "IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from ray.matchbox import Trainer\n",
    "from torchvision.models.densenet import densenet121 as feature_extractor\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 224\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((SCALE,SCALE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_set = ImageFolder(IMG,transform = transform, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train /Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_set = ImageFolder(IMG,transform = transform, )\n",
    "val_set = ImageFolder(IMG,transform = transform, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pick = np.random.rand(len(img_set.samples))>0.95\n",
    "trn_pick = ~val_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_set.samples = np.array(img_set.samples)[trn_pick].tolist()\n",
    "val_set.samples = np.array(img_set.samples)[val_pick].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_set.imgs = trn_set.samples\n",
    "val_set.imgs = val_set.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96030, 4970)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_set),len(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[-0.1314, -0.0458,  0.0227,  ...,  0.0056,  0.0398, -0.0629],\n",
       "           [-0.1143, -0.0458, -0.1314,  ...,  0.0056, -0.0972, -0.1486],\n",
       "           [-0.1143, -0.1657, -0.1314,  ..., -0.0287, -0.0972, -0.0972],\n",
       "           ...,\n",
       "           [-0.4739, -0.5082,  0.0056,  ..., -1.0048, -1.0562, -0.9534],\n",
       "           [-0.6109, -0.6109, -0.5253,  ..., -0.9363, -0.9534, -0.9192],\n",
       "           [-0.6623, -0.6965, -0.6452,  ..., -0.9020, -0.8849, -0.8678]],\n",
       " \n",
       "          [[-1.0903, -1.0553, -1.0028,  ..., -1.0553, -1.0378, -1.1429],\n",
       "           [-1.0728, -1.0378, -1.1429,  ..., -1.0553, -1.1604, -1.1779],\n",
       "           [-1.0378, -1.1779, -1.1253,  ..., -1.0728, -1.1253, -1.0903],\n",
       "           ...,\n",
       "           [-1.4230, -1.4580, -0.8803,  ..., -1.8782, -1.8957, -1.7731],\n",
       "           [-1.4930, -1.5280, -1.4580,  ..., -1.8606, -1.8782, -1.8431],\n",
       "           [-1.5105, -1.5980, -1.5980,  ..., -1.8431, -1.8256, -1.8256]],\n",
       " \n",
       "          [[-1.2293, -1.1770, -1.1421,  ..., -1.3339, -1.3164, -1.4384],\n",
       "           [-1.2293, -1.2119, -1.2990,  ..., -1.3339, -1.4384, -1.4733],\n",
       "           [-1.2293, -1.3513, -1.2990,  ..., -1.3513, -1.4036, -1.4036],\n",
       "           ...,\n",
       "           [-1.5256, -1.6302, -1.4907,  ..., -1.8044, -1.8044, -1.7173],\n",
       "           [-1.6999, -1.6650, -1.6824,  ..., -1.7870, -1.7696, -1.7522],\n",
       "           [-1.8044, -1.7696, -1.6302,  ..., -1.7696, -1.7347, -1.7173]]],\n",
       " \n",
       " \n",
       "         [[[-0.0629, -0.0629, -0.0116,  ..., -1.4329, -1.4158, -1.5014],\n",
       "           [-0.0801, -0.0972, -0.0458,  ..., -1.4672, -1.3987, -1.4843],\n",
       "           [-0.1143, -0.1486, -0.1657,  ..., -1.5014, -1.3987, -1.4843],\n",
       "           ...,\n",
       "           [ 0.8104,  0.8447,  0.8447,  ...,  1.8208,  1.7352,  1.6667],\n",
       "           [ 0.8104,  0.8447,  0.7762,  ...,  1.7694,  1.6838,  1.6153],\n",
       "           [ 0.8276,  0.8276,  0.8276,  ...,  1.7523,  1.6667,  1.5982]],\n",
       " \n",
       "          [[-0.4601, -0.4776, -0.4251,  ..., -1.6155, -1.6331, -1.7381],\n",
       "           [-0.4601, -0.4776, -0.4426,  ..., -1.6681, -1.6155, -1.7206],\n",
       "           [-0.4601, -0.4951, -0.5301,  ..., -1.7556, -1.6506, -1.7381],\n",
       "           ...,\n",
       "           [-0.0399, -0.0049,  0.0126,  ...,  1.8683,  1.7808,  1.7108],\n",
       "           [-0.0399, -0.0049, -0.0574,  ...,  1.8333,  1.7458,  1.6933],\n",
       "           [-0.0224, -0.0224, -0.0049,  ...,  1.8333,  1.7458,  1.6933]],\n",
       " \n",
       "          [[-0.7587, -0.7587, -0.7238,  ..., -1.3687, -1.3861, -1.4733],\n",
       "           [-0.7238, -0.7413, -0.7064,  ..., -1.4384, -1.3861, -1.4733],\n",
       "           [-0.7064, -0.7064, -0.7238,  ..., -1.5256, -1.4210, -1.5256],\n",
       "           ...,\n",
       "           [-0.6018, -0.5670, -0.5844,  ...,  1.5245,  1.4025,  1.3154],\n",
       "           [-0.6018, -0.5844, -0.6541,  ...,  1.5071,  1.4025,  1.2980],\n",
       "           [-0.5844, -0.6018, -0.6018,  ...,  1.5071,  1.4025,  1.2980]]]]),\n",
       " ('92', '43')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = iter(DataLoader(trn_set,batch_size=2,shuffle=True))\n",
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        a pytorch version of Flatten layer\n",
    "        \"\"\"\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(x):\n",
    "    \"\"\"\n",
    "    Arg max of a torch tensor (2 dimensional, dim=1)\n",
    "    :param x:  torch tensor\n",
    "    :return: index the of the max\n",
    "    \"\"\"\n",
    "    return torch.max(x, dim=1)[1]\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_pred: predition of y (will be argmaxed)\n",
    "    :param y_true: true label of y (index)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return (argmax(y_pred) == y_true).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,path):\n",
    "    \"\"\"\n",
    "    model:pytorch model\n",
    "    path:save to path, end with pkl\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "def load_model(model,path):\n",
    "    model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "conv_model = feature_extractor(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = conv_model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_WIDTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  top_half(nn.Module):\n",
    "    def __init__(self,ks = 7):\n",
    "        super(top_half,self).__init__()\n",
    "        self.ks = ks\n",
    "        self.classifier = nn.Linear(FEATURE_WIDTH,len(img_set.classes),bias = True)\n",
    "        self.flatten = Flatten()\n",
    "        nn.init.constant_(self.classifier.weight, 1)\n",
    "        nn.init.constant_(self.classifier.bias, 0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(x,inplace=True)\n",
    "        x = F.avg_pool2d(x,kernel_size = self.ks, stride = 1 )\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_half_  = top_half()\n",
    "\n",
    "if CUDA:\n",
    "    top_half_.cuda()\n",
    "    conv_layers.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(*args,**kwargs):\n",
    "    \"\"\"\n",
    "    single training step, \n",
    "    take in data, spit out loss/ metric\n",
    "    and \n",
    "    \"\"\"\n",
    "    x,y = args[0]\n",
    "    y = torch.LongTensor(np.array(y).astype(int))\n",
    "    if CUDA:\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "    opt.zero_grad()\n",
    "    y_ = top_half_(conv_layers(x))\n",
    "    \n",
    "    loss = loss_func(y_,y)\n",
    "    acc = accuracy(y_,y)\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    return {\"loss\":loss.item(),\n",
    "            \"acc\":acc.item()}\n",
    "\n",
    "def val_action(*args,**kwargs):\n",
    "    x,y = args[0]\n",
    "    y = torch.LongTensor(np.array(y).astype(int))\n",
    "    \n",
    "    if CUDA:\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "    y_ = top_half_(conv_layers(x))\n",
    "    \n",
    "    loss = loss_func(y_,y)\n",
    "    acc = accuracy(y_,y)\n",
    "    \n",
    "    return {\"loss\":loss.item(),\n",
    "            \"acc\":acc.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = Adam(top_half_.parameters())\n",
    "\n",
    "trainer = Trainer(trn_set, val_dataset = val_set, batch_size = 32, print_on = 5)\n",
    "\n",
    "trainer.action = action\n",
    "trainer.val_action = val_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model(dense_conv2,\"food_dense_conv2.0.0.1.npy\")\n",
    "# load_model(top_half_,\"food_top.0.0.1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⭐[ep_0_i_2999]\tacc\t0.506✨\tloss\t1.678: 100%|██████████| 3001/3001 [48:23<00:00,  1.03it/s]\n",
      "😎[val_ep_0_i_33]\tacc\t0.548😂\tloss\t1.832:  22%|██▏       | 34/156 [00:21<01:16,  1.60it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(conv_layers,\"convlayers2.0.0.1.npy\")\n",
    "save_model(top_half_,\"food_top.0.0.1.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please work on at least 2 of the following challenge\n",
    "\n",
    "1. Optimize all the layers instead of only linear classifier\n",
    "2. Optimize the last convblock(the conv block close the linear layer) and linear classifier\n",
    "3. Try other image classify datasets, like [monekey image set](https://www.kaggle.com/slothkong/10-monkey-species) or [flower classifying problem](https://www.kaggle.com/alxmamaev/flowers-recognition) or [blood cell images](https://www.kaggle.com/paultimothymooney/blood-cells). You'll pretty soon find out convolutional neural network is a universal tool for this kind of problem\n",
    "4. Try keras to work out a better accuaracy, keras import pretrained model by ```from keras.application import ... ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how we break down a pytorch model to several pytorch models:\n",
    "\n",
    "```python\n",
    "dense_conv1 = nn.Sequential(*[getattr(conv_model.features,nn_name) for nn_name in [\"conv0\",\"norm0\",\"relu0\",\"pool0\",\"denseblock1\",\"transition1\",\n",
    "                                                                                   \"denseblock2\",\"transition2\",\"denseblock3\",\"transition3\",]])\n",
    "\n",
    "dense_conv2 = nn.Sequential(*[getattr(conv_model.features,nn_name) for nn_name in [\"denseblock4\",\"norm5\"]])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
