{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Image Classifcation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going learn [food images](https://www.kaggle.com/kmader/food41) from kaggle\n",
    "\n",
    "### Use the kaggle-cli to download the image\n",
    "\n",
    "```kaggle datasets download -d kmader/food41```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA  = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.environ[\"HOME\"]\n",
    "DATA  = HOME+\"/.kaggle/datasets/kmader/food41/\"\n",
    "META = DATA + \"meta/meta/\"\n",
    "IMG = DATA+\"images/\"\n",
    "VERSION = \"0.0.4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from ray.matchbox import Trainer\n",
    "from torchvision.models.densenet import densenet121 as feature_extractor\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 224\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((SCALE,SCALE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_set = ImageFolder(IMG,transform = transform, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train /Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_set = ImageFolder(IMG,transform = transform, )\n",
    "val_set = ImageFolder(IMG,transform = transform, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pick = np.random.rand(len(img_set.samples))>0.8\n",
    "trn_pick = ~val_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_set.samples = np.array(img_set.samples)[trn_pick].tolist()\n",
    "val_set.samples = np.array(img_set.samples)[val_pick].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_set.imgs = trn_set.samples\n",
    "val_set.imgs = val_set.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80755, 20245)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_set),len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = iter(DataLoader(trn_set,batch_size=2,shuffle=True))\n",
    "# next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        a pytorch version of Flatten layer\n",
    "        \"\"\"\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def argmax(x):\n",
    "    \"\"\"\n",
    "    Arg max of a torch tensor (2 dimensional, dim=1)\n",
    "    :param x:  torch tensor\n",
    "    :return: index the of the max\n",
    "    \"\"\"\n",
    "    return torch.max(x, dim=1)[1]\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    :param y_pred: predition of y (will be argmaxed)\n",
    "    :param y_true: true label of y (index)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return (argmax(y_pred) == y_true).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,path):\n",
    "    \"\"\"\n",
    "    model:pytorch model\n",
    "    path:save to path, end with pkl\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "def load_model(model,path):\n",
    "    model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "conv_model = feature_extractor(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = conv_model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_WIDTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  top_half(nn.Module):\n",
    "    def __init__(self,ks = 7):\n",
    "        super(top_half,self).__init__()\n",
    "        self.ks = ks\n",
    "        self.classifier = nn.Linear(FEATURE_WIDTH,len(img_set.classes),bias = True)\n",
    "        self.flatten = Flatten()\n",
    "        nn.init.constant_(self.classifier.weight, 1)\n",
    "        nn.init.constant_(self.classifier.bias, 0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(x,inplace=True)\n",
    "        x = F.avg_pool2d(x,kernel_size = self.ks, stride = 1 )\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_half_  = top_half()\n",
    "\n",
    "if CUDA:\n",
    "    top_half_.cuda()\n",
    "    conv_layers.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(*args,**kwargs):\n",
    "    \"\"\"\n",
    "    single training step, \n",
    "    take in data, spit out loss/ metric\n",
    "    and \n",
    "    \"\"\"\n",
    "    x,y = args[0]\n",
    "    y = torch.LongTensor(np.array(y).astype(int))\n",
    "    if CUDA:\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "    opt.zero_grad()\n",
    "    y_ = top_half_(conv_layers(x))\n",
    "    \n",
    "    loss = loss_func(y_,y)\n",
    "    acc = accuracy(y_,y)\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if kwargs[\"ite\"] % 10 ==9:\n",
    "        save_model(conv_layers,\"convlayers2.%s.npy\"%(VERSION))\n",
    "        save_model(top_half_,\"food_top.%s.npy\"%(VERSION))\n",
    "    \n",
    "    return {\"loss\":loss.item(),\n",
    "            \"acc\":acc.item()}\n",
    "\n",
    "def val_action(*args,**kwargs):\n",
    "    x,y = args[0]\n",
    "    y = torch.LongTensor(np.array(y).astype(int))\n",
    "    \n",
    "    if CUDA:\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "    y_ = top_half_(conv_layers(x))\n",
    "    \n",
    "    loss = loss_func(y_,y)\n",
    "    acc = accuracy(y_,y)\n",
    "    \n",
    "    return {\"loss\":loss.item(),\n",
    "            \"acc\":acc.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = Adam(list(top_half_.parameters()) + list(conv_layers.parameters()))\n",
    "\n",
    "trainer = Trainer(trn_set, val_dataset = val_set, batch_size = 32, print_on = 5)\n",
    "\n",
    "trainer.action = action\n",
    "trainer.val_action = val_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model(dense_conv2,\"food_dense_conv2.0.0.1.npy\")\n",
    "# load_model(top_half_,\"food_top.0.0.1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚≠ê[ep_0_i_2519]\tacc\t0.388‚ú®\tloss\t2.457: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2524/2524 [41:57<00:00,  1.00it/s]\n",
      "üòé[val_ep_0_i_632]\tacc\t0.400üòÇ\tloss\t2.394: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 633/633 [05:57<00:00,  1.77it/s]\n",
      "‚≠ê[ep_1_i_2519]\tacc\t0.613‚ú®\tloss\t1.588: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2524/2524 [42:48<00:00,  1.02s/it]\n",
      "üòé[val_ep_1_i_632]\tacc\t0.534üòÇ\tloss\t1.786: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 633/633 [07:19<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = resnet101(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.,  0.,  0.,  ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fc = nn.Linear(2048,len(img_set.imgs),bias = True)\n",
    "nn.init.constant_(conv_model.fc.weight,1)\n",
    "nn.init.constant_(conv_model.fc.bias,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "if CUDA:\n",
    "    conv_model.cuda()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = Adam(list(conv_model.layer4.parameters())+list(conv_model.fc.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(*args,**kwargs):\n",
    "    \"\"\"\n",
    "    single training step, \n",
    "    take in data, spit out loss/ metric\n",
    "    and \n",
    "    \"\"\"\n",
    "    x,y = args[0]\n",
    "    y = torch.LongTensor(np.array(y).astype(int))\n",
    "    if CUDA:\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "    opt.zero_grad()\n",
    "    y_ = conv_model(x)\n",
    "    \n",
    "    loss = loss_func(y_,y)\n",
    "    acc = accuracy(y_,y)\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if kwargs[\"ite\"] % 10 ==9:\n",
    "        save_model(conv_model,\"food_rn101.%s.npy\"%(VERSION))\n",
    "    \n",
    "    return {\"loss\":loss.item(),\n",
    "            \"acc\":acc.item()}\n",
    "\n",
    "def val_action(*args,**kwargs):\n",
    "    x,y = args[0]\n",
    "    y = torch.LongTensor(np.array(y).astype(int))\n",
    "    \n",
    "    if CUDA:\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "    y_ = conv_model(x)\n",
    "    \n",
    "    loss = loss_func(y_,y)\n",
    "    acc = accuracy(y_,y)\n",
    "    \n",
    "    return {\"loss\":loss.item(),\n",
    "            \"acc\":acc.item()}\n",
    "\n",
    "trainer = Trainer(trn_set, val_dataset = val_set, batch_size = 16, print_on = 5)\n",
    "\n",
    "trainer.action = action\n",
    "trainer.val_action = val_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚≠ê[ep_0_i_5044]\tacc\t0.713‚ú®\tloss\t1.227: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5048/5048 [1:38:33<00:00,  1.17s/it]\n",
      "üòé[val_ep_0_i_1265]\tacc\t0.651üòÇ\tloss\t1.328: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1266/1266 [08:54<00:00,  2.37it/s]\n",
      "‚≠ê[ep_1_i_2919]\tacc\t0.738‚ú®\tloss\t0.919:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 2921/5048 [55:58<40:45,  1.15s/it]  "
     ]
    }
   ],
   "source": [
    "trainer.train(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please work on at least 2 of the following challenge\n",
    "\n",
    "1. Optimize all the layers instead of only linear classifier\n",
    "2. Optimize the last convblock(the conv block close the linear layer) and linear classifier\n",
    "3. Try other image classify datasets, like [monekey image set](https://www.kaggle.com/slothkong/10-monkey-species) or [flower classifying problem](https://www.kaggle.com/alxmamaev/flowers-recognition) or [blood cell images](https://www.kaggle.com/paultimothymooney/blood-cells). You'll pretty soon find out convolutional neural network is a universal tool for this kind of problem\n",
    "4. Try keras to work out a better accuaracy, keras import pretrained model by ```from keras.application import ... ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how we break down a pytorch model to several pytorch models:\n",
    "\n",
    "```python\n",
    "dense_conv1 = nn.Sequential(*[getattr(conv_model.features,nn_name) for nn_name in [\"conv0\",\"norm0\",\"relu0\",\"pool0\",\"denseblock1\",\"transition1\",\n",
    "                                                                                   \"denseblock2\",\"transition2\",\"denseblock3\",\"transition3\",]])\n",
    "\n",
    "dense_conv2 = nn.Sequential(*[getattr(conv_model.features,nn_name) for nn_name in [\"denseblock4\",\"norm5\"]])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
